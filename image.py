# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LMKQh6RwSoj1sukUZaW-ZRa_U0winHZG
"""

from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer
import signal
import sys
from PIL import Image
import torch
import os
import json
from collections import defaultdict

def exit_gracefully(signum, frame):
    signal.signal(signal.SIGINT, original_sigint)
    sys.exit(1)

model = VisionEncoderDecoderModel.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
processor = ViTImageProcessor.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
tokenizer = AutoTokenizer.from_pretrained("nlpconnect/vit-gpt2-image-captioning")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

def saveTextFile(text):
    try:
        print(text)
        with open("output_data.txt", "w+") as text_file:
            text_file.write(text)
    except Exception as e:
        print("Exception occurred\n")
        print(e)

def read_image():
    path_to_file = '/content/img1.jpg'
    image = Image.open(path_to_file)
    if image.mode != "RGB":
        image = image.convert(mode="RGB")
    return [image]

max_length = 16
num_beams = 4
gen_kwargs = {"max_length": max_length, "num_beams": num_beams}

def predict_step(images):
    pixel_values = processor(images=images, return_tensors="pt").pixel_values
    pixel_values = pixel_values.to(device)

    output_ids = model.generate(pixel_values, **gen_kwargs)

    preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)
    preds = [pred.strip() for pred in preds]
    return preds

def tag_from_data(preds):
    awsstring = "I think it is " + str(preds[0])
    return awsstring

if __name__ == '__main__':
    original_sigint = signal.getsignal(signal.SIGINT)
    signal.signal(signal.SIGINT, exit_gracefully)
    images = read_image()
    data = predict_step(images)
    text = tag_from_data(data)
    saveTextFile(text)

